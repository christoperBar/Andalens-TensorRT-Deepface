{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9676fcf0-a42c-445e-8f3b-4ea92279c5f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Install Dependensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9293821a-2323-431a-ba43-06b619058486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.9.2 in /usr/local/lib/python3.11/dist-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.9.2) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.19.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting opencv-python==4.9.0.80\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python==4.9.0.80) (2.2.6)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:18\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.12.0.88\n",
      "    Uninstalling opencv-python-4.12.0.88:\n",
      "      Successfully uninstalled opencv-python-4.12.0.88\n",
      "Successfully installed opencv-python-4.9.0.80\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib==3.9.2\n",
    "!pip install gdown\n",
    "!pip install opencv-python==4.9.0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd9fc9da-b536-443b-9b24-80acbea0f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libgl1 is already the newest version (1.4.0-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 99 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y libgl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d752ef31-a98f-4ad9-af95-a327f4b2915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1940 kB]\n",
      "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3253 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1271 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5235 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5519 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3606 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1576 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Fetched 43.0 MB in 13s (3316 kB/s)                                             \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  wget\n",
      "0 upgraded, 1 newly installed, 0 to remove and 94 not upgraded.\n",
      "Need to get 339 kB of archives.\n",
      "After this operation, 950 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 wget amd64 1.21.2-2ubuntu1.1 [339 kB]\n",
      "Fetched 339 kB in 2s (197 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package wget.\n",
      "(Reading database ... 22806 files and directories currently installed.)\n",
      "Preparing to unpack .../wget_1.21.2-2ubuntu1.1_amd64.deb ...\n",
      "Unpacking wget (1.21.2-2ubuntu1.1) ...\n",
      "Setting up wget (1.21.2-2ubuntu1.1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get update && apt-get install -y wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e26f39-a385-4a9f-9302-ab68def7efd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Reading database ... 22817 files and directories currently installed.)\n",
      "Preparing to unpack cuda-keyring_1.1-1_all.deb ...\n",
      "Unpacking cuda-keyring (1.1-1) over (1.0-1) ...\n",
      "Setting up cuda-keyring (1.1-1) ...\n",
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease              \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!dpkg -i cuda-keyring_1.1-1_all.deb\n",
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933cef90-3753-493b-b5ab-79c50c9eaef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-28 05:47:45--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/libnvonnxparsers8_8.6.1.6-1+cuda11.8_amd64.deb\n",
      "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 23.55.44.101, 23.55.44.103\n",
      "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|23.55.44.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 710054 (693K) [application/x-deb]\n",
      "Saving to: ‚Äòlibnvonnxparsers8_8.6.1.6-1+cuda11.8_amd64.deb‚Äô\n",
      "\n",
      "libnvonnxparsers8_8 100%[===================>] 693.41K   864KB/s    in 0.8s    \n",
      "\n",
      "2025-08-28 05:47:48 (864 KB/s) - ‚Äòlibnvonnxparsers8_8.6.1.6-1+cuda11.8_amd64.deb‚Äô saved [710054/710054]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/libnvonnxparsers8_8.6.1.6-1+cuda11.8_amd64.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf752e06-a3bb-4821-932e-42846e9b2cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libnvparsers8.\n",
      "(Reading database ... 22849 files and directories currently installed.)\n",
      "Preparing to unpack libnvparsers8_8.6.1.6-1+cuda11.8_amd64.deb ...\n",
      "Unpacking libnvparsers8 (8.6.1.6-1+cuda11.8) ...\n",
      "Selecting previously unselected package libnvonnxparsers8.\n",
      "Preparing to unpack libnvonnxparsers8_8.6.1.6-1+cuda11.8_amd64.deb ...\n",
      "Unpacking libnvonnxparsers8 (8.6.1.6-1+cuda11.8) ...\n",
      "Preparing to unpack python3-libnvinfer_8.6.1.6-1+cuda11.8_amd64.deb ...\n",
      "Unpacking python3-libnvinfer (8.6.1.6-1+cuda11.8) over (8.6.1.6-1+cuda11.8) ...\n",
      "Setting up libnvparsers8 (8.6.1.6-1+cuda11.8) ...\n",
      "Setting up libnvonnxparsers8 (8.6.1.6-1+cuda11.8) ...\n",
      "Setting up python3-libnvinfer (8.6.1.6-1+cuda11.8) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
     ]
    }
   ],
   "source": [
    "!dpkg -i libnvinfer8_8.6.1.6-1+cuda11.8_amd64.deb\\\n",
    "            libnvinfer-vc-plugin8_8.6.1.6-1+cuda11.8_amd64.deb\\\n",
    "            libnvinfer-plugin8_8.6.1.6-1+cuda11.8_amd64.deb\\\n",
    "            libnvparsers8_8.6.1.6-1+cuda11.8_amd64.deb \\\n",
    "            libnvonnxparsers8_8.6.1.6-1+cuda11.8_amd64.deb\\\n",
    "            python3-libnvinfer_8.6.1.6-1+cuda11.8_amd64.deb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ddcd4-4143-4ace-8682-3ca284fb4db0",
   "metadata": {},
   "source": [
    "# Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b190ce-3144-4c25-857d-6b23b61ae45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d52f405-da19-460f-8814-e06a7b8c94ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.13\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26678ed1-0440-4cb1-9e7b-861978e04090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  3 05:00:58 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.51.02              Driver Version: 576.02         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   58C    P8             13W /   91W |     272MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32df90d4-9675-41fd-9e44-48f886cbc8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii  libnvinfer-plugin8              8.6.1.6-1+cuda11.8                      amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer-vc-plugin8           8.6.1.6-1+cuda11.8                      amd64        TensorRT vc-plugin library\n",
      "ii  libnvinfer8                     8.6.1.6-1+cuda11.8                      amd64        TensorRT runtime libraries\n",
      "ii  python3-libnvinfer              8.6.1.6-1+cuda11.8                      amd64        Python 3 bindings for TensorRT standard runtime\n"
     ]
    }
   ],
   "source": [
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3c27b7-6ad0-4b50-9ace-ee9df02a161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 05:01:00.835171: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-03 05:01:00.835246: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-03 05:01:00.836520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.14.0\n",
      "Num GPUs Available: 1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad6de8f-56f7-48ba-a745-2f3544f7ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux 0d07041e5940 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d45e6ea-5061-491a-a783-454a6dff1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "Tensorflow version:  2.14.0\n",
      "TensorRT version: \n",
      "ii  libnvinfer-plugin8              8.6.1.6-1+cuda11.8                      amd64        TensorRT plugin libraries\n",
      "ii  libnvinfer-vc-plugin8           8.6.1.6-1+cuda11.8                      amd64        TensorRT vc-plugin library\n",
      "ii  libnvinfer8                     8.6.1.6-1+cuda11.8                      amd64        TensorRT runtime libraries\n",
      "ii  python3-libnvinfer              8.6.1.6-1+cuda11.8                      amd64        Python 3 bindings for TensorRT standard runtime\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "print(\"Python version: \",sys.version)\n",
    "print(\"Tensorflow version: \", tf.version.VERSION)\n",
    "# check TensorRT version\n",
    "print(\"TensorRT version: \")\n",
    "!dpkg -l | grep nvinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873ae7b3-2e63-4a42-a7aa-75ca7739e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Core GPU Present: True\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def check_tensor_core_gpu_present():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    for line in local_device_protos:\n",
    "        if \"compute capability\" in str(line):\n",
    "            compute_capability = float(line.physical_device_desc.split(\"compute capability: \")[-1])\n",
    "            if compute_capability>=7.0:\n",
    "                return True\n",
    "\n",
    "print(\"Tensor Core GPU Present:\", check_tensor_core_gpu_present())\n",
    "tensor_core_gpu = check_tensor_core_gpu_present()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f65356-1c62-443a-8ae1-47e91e4ac056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from typing import Any, Dict, List, Union\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9484b4-7dce-4d97-8daf-0c522a7deb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenetlib.Facenet_standalone import InceptionResNetV1\n",
    "from retina import detect_faces, create_retinaface_model, detect_faces_batch\n",
    "from facenetlib import preprocessing, image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52341a1a-9845-4b3d-af08-2b8030fa79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACENET_TRT_DIR = \"saved_model/facenet512\"\n",
    "RETINA_TRT_DIR  = \"saved_model/retinaface_saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d859aa9-b758-4468-8caa-8092d305c3f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Convert TensorRT (Facenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2782b0b-e593-49b6-a3ea-afc751ecef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to TF-TRT FP16...\n",
      "INFO:tensorflow:Linked TensorRT version: (8, 4, 3)\n",
      "INFO:tensorflow:Loaded TensorRT version: (8, 6, 1)\n",
      "INFO:tensorflow:Loaded TensorRT 8.6.1 and linked TensorFlow against TensorRT 8.4.3. This is supported because TensorRT minor/patch upgrades are backward compatible.\n",
      "INFO:tensorflow:Clearing prior device assignments in loaded saved model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction pruned(*, input_2) at 0x7BAB161DB010>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Converting to TF-TRT FP16...')\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "    precision_mode=trt.TrtPrecisionMode.FP16,\n",
    ")\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "   input_saved_model_dir=FACENET_TRT_DIR, conversion_params=conversion_params)\n",
    "converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e4612c5-8b11-468a-9f81-2fa88d6b56dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     \n",
      "================================================================================================================================================================\n",
      "TRTEngineOp_000_000              device:GPU:0  967     1             1             ['float32']        ['float32']        [[-1, 160, 160 ... [[-1, 512]]       \n",
      "\n",
      "\t- AddV2: 22x\n",
      "\t- BiasAdd: 21x\n",
      "\t- ConcatV2: 23x\n",
      "\t- Const: 501x\n",
      "\t- Conv2D: 132x\n",
      "\t- FusedBatchNormV3: 111x\n",
      "\t- MatMul: 1x\n",
      "\t- MaxPool: 3x\n",
      "\t- Mean: 1x\n",
      "\t- Mul: 21x\n",
      "\t- Relu: 131x\n",
      "\n",
      "================================================================================================================================================================\n",
      "[*] Total number of TensorRT engines: 1\n",
      "[*] % of OPs Converted: 99.79% [967/969]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50b3293f-0b1d-439a-a7bc-9081a88b627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find TRTEngineOp_000_000 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: facenet512_saved_model_TFTRT_FP16/assets\n",
      "Done Converting to TF-TRT FP16\n"
     ]
    }
   ],
   "source": [
    "converter.save(output_saved_model_dir='facenet512_saved_model_TFTRT_FP16')\n",
    "print('Done Converting to TF-TRT FP16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e241f-c194-4bf7-9e1a-0bc4e79d6e0d",
   "metadata": {},
   "source": [
    "# Load + Warmup (facenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f2616c-6f01-4320-81ba-f593466222c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Warming up...\n",
      "\n",
      "Benchmark Original TF Model...\n",
      "Original Model Avg Time: 0.0091 sec per inference\n",
      "\n",
      "Benchmark TensorRT FP16 Model...\n",
      "TensorRT FP16 Avg Time: 0.0020 sec per inference\n",
      "\n",
      "Tip: Jalankan 'nvidia-smi' di terminal lain untuk lihat GPU usage realtime üöÄ\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Path model original dan model TensorRT FP16\n",
    "ORIG_MODEL_DIR = FACENET_TRT_DIR\n",
    "TRT_MODEL_DIR = \"facenet512_saved_model_TFTRT_FP16\"\n",
    "\n",
    "# ======================\n",
    "# 0. Cek GPU tersedia\n",
    "# ======================\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     print(\"‚úÖ GPU detected:\", gpus)\n",
    "# else:\n",
    "#     print(\"‚ùå No GPU detected, fallback CPU\")\n",
    "\n",
    "# Aktifkan log placement supaya tahu ops jalan di GPU / CPU\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# 1. Load model original & TensorRT FP16\n",
    "print(\"Loading models...\")\n",
    "orig_model = tf.saved_model.load(ORIG_MODEL_DIR)\n",
    "trt_model = tf.saved_model.load(TRT_MODEL_DIR)\n",
    "\n",
    "orig_infer = orig_model.signatures[\"serving_default\"]\n",
    "trt_infer = trt_model.signatures[\"serving_default\"]\n",
    "\n",
    "# 2. Siapkan input dummy\n",
    "dummy_input = np.random.rand(1, 160, 160, 3).astype(np.float32)\n",
    "input_tensor = tf.convert_to_tensor(dummy_input)\n",
    "\n",
    "# 3. Warm-up (supaya cache GPU & TensorRT engine siap)\n",
    "print(\"Warming up...\")\n",
    "for _ in range(10):\n",
    "    _ = orig_infer(input_tensor)\n",
    "    _ = trt_infer(input_tensor)\n",
    "\n",
    "# 4. Benchmark Original Model\n",
    "print(\"\\nBenchmark Original TF Model...\")\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = orig_infer(input_tensor)\n",
    "end = time.time()\n",
    "orig_time = (end - start) / 100\n",
    "print(f\"Original Model Avg Time: {orig_time:.4f} sec per inference\")\n",
    "\n",
    "# 5. Benchmark TensorRT FP16 Model\n",
    "print(\"\\nBenchmark TensorRT FP16 Model...\")\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = trt_infer(input_tensor)\n",
    "end = time.time()\n",
    "trt_time = (end - start) / 100\n",
    "print(f\"TensorRT FP16 Avg Time: {trt_time:.4f} sec per inference\")\n",
    "\n",
    "# ======================\n",
    "# 6. Cek GPU usage runtime\n",
    "# ======================\n",
    "print(\"\\nTip: Jalankan 'nvidia-smi' di terminal lain untuk lihat GPU usage realtime üöÄ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e61e57-7874-4282-965f-4091b83a6049",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Convert TensorRT (Retinaface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751e5e62-0fc9-4dd7-9bf9-ffbfb13d61ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting RetinaFace to TF-TRT FP16...\n",
      "INFO:tensorflow:Linked TensorRT version: (8, 4, 3)\n",
      "INFO:tensorflow:Loaded TensorRT version: (8, 6, 1)\n",
      "INFO:tensorflow:Loaded TensorRT 8.6.1 and linked TensorFlow against TensorRT 8.4.3. This is supported because TensorRT minor/patch upgrades are backward compatible.\n",
      "INFO:tensorflow:Clearing prior device assignments in loaded saved model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction pruned(*, input) at 0x73850B7BCF50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Converting RetinaFace to TF-TRT FP16...\")\n",
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "    precision_mode=trt.TrtPrecisionMode.FP16\n",
    ")\n",
    "\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=RETINA_TRT_DIR,\n",
    "    conversion_params=conversion_params\n",
    ")\n",
    "converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "514c2411-7cc2-4542-a09c-f9e5d9222583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     \n",
      "================================================================================================================================================================\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "TRTEngineOp_000_000              device:GPU:0  543     1             6             ['float32']        ['float32', 'f ... [[-1, -1, -1, 3]]  [[-1, -1, -1,  ...\n",
      "\n",
      "\t- AddV2: 16x\n",
      "\t- BiasAdd: 11x\n",
      "\t- ConcatV2: 1x\n",
      "\t- Const: 314x\n",
      "\t- Conv2D: 64x\n",
      "\t- FusedBatchNormV3: 59x\n",
      "\t- MaxPool: 1x\n",
      "\t- Pad: 21x\n",
      "\t- Relu: 56x\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "TRTEngineOp_000_001              device:GPU:0  77      1             4             ['float32']        ['float32', 'f ... [[-1, -1, -1,  ... [[-1, -1, -1,  ...\n",
      "\n",
      "\t- BiasAdd: 9x\n",
      "\t- ConcatV2: 1x\n",
      "\t- Const: 44x\n",
      "\t- Conv2D: 9x\n",
      "\t- FusedBatchNormV3: 6x\n",
      "\t- Pad: 4x\n",
      "\t- Relu: 4x\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "TRTEngineOp_000_002              device:GPU:0  77      1             3             ['float32']        ['float32', 'f ... [[-1, -1, -1,  ... [[-1, -1, -1,  ...\n",
      "\n",
      "\t- BiasAdd: 9x\n",
      "\t- ConcatV2: 1x\n",
      "\t- Const: 44x\n",
      "\t- Conv2D: 9x\n",
      "\t- FusedBatchNormV3: 6x\n",
      "\t- Pad: 4x\n",
      "\t- Relu: 4x\n",
      "\n",
      "================================================================================================================================================================\n",
      "[*] Total number of TensorRT engines: 3\n",
      "[*] % of OPs Converted: 83.87% [697/831]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "converter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "265e072b-0f5c-457f-842d-c5f27da8583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Could not find TRTEngineOp_000_001 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_000_002 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: retinaface_saved_model_TFTRT_FP16/assets\n",
      "Done Converting RetinaFace to TF-TRT FP16\n"
     ]
    }
   ],
   "source": [
    "converter.save(output_saved_model_dir=\"retinaface_saved_model_TFTRT_FP16\")\n",
    "print(\"Done Converting RetinaFace to TF-TRT FP16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15e96b-8675-45f4-93a3-240543329640",
   "metadata": {},
   "source": [
    "# Load + Warmup (Retina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d16d4b-27c2-443e-95ae-80b89488745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading optimized RetinaFace model (TF-TRT FP16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 05:01:15.854651: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:87] DefaultLogger 3: [runtime.cpp::~Runtime::346] Error Code 3: API Usage Error (Parameter check failed at: runtime/rt/runtime.cpp::~Runtime::346, condition: mEngineCounter.use_count() == 1. Destroying a runtime before destroying deserialized engines created by the runtime leads to undefined behavior.\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Warming up RetinaFace...\n",
      "\n",
      "[INFO] Benchmarking RetinaFace TF-TRT FP16...\n",
      "[RESULT] RetinaFace TF-TRT FP16 Avg Time: 0.0218 sec per inference\n",
      "\n",
      "Tip: Jalankan 'nvidia-smi' di terminal lain untuk monitor GPU usage üöÄ\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# 0. Path model RetinaFace TF-TRT\n",
    "# ======================\n",
    "RETINAFACE_TRT_FP16_DIR = \"retinaface_saved_model_TFTRT_FP16\"\n",
    "\n",
    "# 1. Load RetinaFace TF-TRT Model\n",
    "print(\"[INFO] Loading optimized RetinaFace model (TF-TRT FP16)...\")\n",
    "detector = tf.saved_model.load(RETINAFACE_TRT_FP16_DIR)\n",
    "infer_detector = detector.signatures[\"serving_default\"]\n",
    "\n",
    "# 2. Siapkan input dummy (ubah sesuai input model RetinaFace)\n",
    "#   RetinaFace biasanya pakai 640x640x3, tapi cek di modelmu\n",
    "dummy_input = np.random.rand(1, 1536, 1024, 3).astype(np.float32)\n",
    "input_tensor = tf.convert_to_tensor(dummy_input)\n",
    "\n",
    "# 3. Warm-up (biar TensorRT engine kebentuk & cache GPU siap)\n",
    "print(\"[INFO] Warming up RetinaFace...\")\n",
    "for _ in range(5):\n",
    "    _ = infer_detector(input_tensor)\n",
    "\n",
    "# 4. Benchmark RetinaFace TF-TRT FP16\n",
    "print(\"\\n[INFO] Benchmarking RetinaFace TF-TRT FP16...\")\n",
    "N = 50  # jumlah loop benchmark\n",
    "start = time.time()\n",
    "for _ in range(N):\n",
    "    _ = infer_detector(input_tensor)\n",
    "end = time.time()\n",
    "\n",
    "avg_time = (end - start) / N\n",
    "print(f\"[RESULT] RetinaFace TF-TRT FP16 Avg Time: {avg_time:.4f} sec per inference\")\n",
    "\n",
    "print(\"\\nTip: Jalankan 'nvidia-smi' di terminal lain untuk monitor GPU usage üöÄ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313aac5-8fda-4157-a362-98b5011f5e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gapake TensorRT (Facenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a9980fd-88b1-45a9-9df8-e41941de2e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Facenet512...\n",
      "[INFO] Loading RetinaFace...\n",
      "Building RetinaFace model architecture...\n",
      "Loading weights into model...\n",
      "Weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Loading Facenet512...\")\n",
    "embedder = InceptionResNetV1(dimension=512)\n",
    "embedder.load_weights(\"facenetlib/facenet512_weights.h5\")\n",
    "\n",
    "print(\"[INFO] Loading RetinaFace...\")\n",
    "detector = create_retinaface_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965c6583-9345-4031-853d-14ab1e0175d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def batch_represent(\n",
    "    img_paths: List[str],\n",
    "    threshold: float = 0.9,\n",
    "    normalization: str = \"base\"\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Deteksi semua wajah dari setiap gambar, proses embedding dalam batch,\n",
    "    dan return hasil per file, sambil print waktu per langkah.\n",
    "    \"\"\"\n",
    "    t0_total = time.perf_counter()\n",
    "\n",
    "    # 1. Load semua gambar\n",
    "    t0 = time.perf_counter()\n",
    "    imgs_rgb = []\n",
    "    imgs_bgr = []\n",
    "    for p in img_paths:\n",
    "        img_rgb, _ = image_utils.load_image(p)\n",
    "        imgs_rgb.append(img_rgb)\n",
    "        imgs_bgr.append(img_rgb[:, :, ::-1])  # BGR untuk detektor\n",
    "    print(f\"[TIMING] Load images: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    # 2. Deteksi wajah & kumpulkan crop\n",
    "    t0 = time.perf_counter()\n",
    "    all_faces = []\n",
    "    meta = []\n",
    "    for idx, (img_path, img_bgr, img_rgb) in enumerate(zip(img_paths, imgs_bgr, imgs_rgb)):\n",
    "        detections = detect_faces(\n",
    "            img_path=img_bgr,\n",
    "            threshold=threshold,\n",
    "            model=detector,\n",
    "        )\n",
    "\n",
    "        # Pastikan format list of dict\n",
    "        if isinstance(detections, dict) and all(isinstance(v, dict) for v in detections.values()):\n",
    "            detections = list(detections.values())\n",
    "\n",
    "        for det in detections:\n",
    "            coords = det[\"facial_area\"]\n",
    "            if isinstance(coords, list):\n",
    "                x1, y1, x2, y2 = coords\n",
    "            else:  # dict\n",
    "                x1, y1 = coords[\"x\"], coords[\"y\"]\n",
    "                x2, y2 = x1 + coords[\"w\"], y1 + coords[\"h\"]\n",
    "\n",
    "            face_conf = det.get(\"confidence\", det.get(\"score\", 0.0))\n",
    "            crop = img_rgb[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            # Resize & normalize\n",
    "            crop_resized = preprocessing.resize_image(crop, target_size=(160, 160))\n",
    "            crop_norm = preprocessing.normalize_input(crop_resized, normalization=normalization)\n",
    "\n",
    "            if crop_norm.ndim == 4 and crop_norm.shape[0] == 1:\n",
    "                crop_norm = crop_norm[0]\n",
    "\n",
    "            all_faces.append(crop_norm)\n",
    "            meta.append({\n",
    "                \"filename\": os.path.basename(img_path),\n",
    "                \"facial_area\": {\n",
    "                    \"x\": int(x1), \"y\": int(y1),\n",
    "                    \"w\": int(x2 - x1), \"h\": int(y2 - y1)\n",
    "                },\n",
    "                \"confidence\": float(face_conf)\n",
    "            })\n",
    "    print(f\"[TIMING] Face detection + preprocessing: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    if not all_faces:\n",
    "        print(\"[INFO] No faces detected.\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Convert ke batch array\n",
    "    t0 = time.perf_counter()\n",
    "    batch_faces = np.array(all_faces)\n",
    "    print(f\"[TIMING] Convert to batch array: {(time.perf_counter() - t0):.6f} sec\")\n",
    "    print(f\"[INFO] Processing {len(batch_faces)} faces in batch...\")\n",
    "\n",
    "    # 4. Sekali forward pass ke Facenet\n",
    "    t0 = time.perf_counter()\n",
    "    embeddings = embedder.predict(batch_faces, verbose=0)\n",
    "    print(f\"[TIMING] Embedding forward pass: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    # 5. Kelompokkan hasil per file\n",
    "    t0 = time.perf_counter()\n",
    "    results_by_file = {}\n",
    "    for m, emb in zip(meta, embeddings):\n",
    "        m[\"embedding\"] = emb.astype(float).tolist()\n",
    "        results_by_file.setdefault(m[\"filename\"], []).append(m)\n",
    "    print(f\"[TIMING] Grouping results: {(time.perf_counter() - t0):.6f} sec\")\n",
    "\n",
    "    print(f\"[TIMING] TOTAL runtime: {(time.perf_counter() - t0_total):.4f} sec\")\n",
    "    return results_by_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2444468-50ec-4aa5-96b8-0a1c8c606480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMING] Load images: 11.9147 sec\n",
      "[TIMING] Face detection + preprocessing: 36.8569 sec\n",
      "[TIMING] Convert to batch array: 0.215885 sec\n",
      "[INFO] Processing 1001 faces in batch...\n",
      "[TIMING] Embedding forward pass: 7.5282 sec\n",
      "[TIMING] Grouping results: 0.022937 sec\n",
      "[TIMING] TOTAL runtime: 56.5396 sec\n",
      "‚úÖ Batch embeddings saved to embedding/embeddings_batchf.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Contoh penggunaan\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"anjay\"\n",
    "    output_json = \"embedding/embeddings_batchf.json\"\n",
    "    os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "\n",
    "    img_files = [\n",
    "        os.path.join(input_folder, f)\n",
    "        for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    results = batch_represent(img_files)\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Batch embeddings saved to {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67290d65-bfd3-46dc-921d-ebbebe47c046",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pake Facenet TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f6f0469-c723-4929-90b9-9fc51c826d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def batch_representfp16(\n",
    "    img_paths: List[str],\n",
    "    threshold: float = 0.9,\n",
    "    normalization: str = \"base\"\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Deteksi semua wajah dari setiap gambar, proses embedding dalam batch,\n",
    "    dan return hasil per file, sambil print waktu per langkah.\n",
    "    \"\"\"\n",
    "    t0_total = time.perf_counter()\n",
    "\n",
    "    # 1. Load semua gambar\n",
    "    t0 = time.perf_counter()\n",
    "    imgs_rgb = []\n",
    "    imgs_bgr = []\n",
    "    for p in img_paths:\n",
    "        img_rgb, _ = image_utils.load_image(p)\n",
    "        imgs_rgb.append(img_rgb)\n",
    "        imgs_bgr.append(img_rgb[:, :, ::-1])  # BGR untuk detektor\n",
    "    print(f\"[TIMING] Load images: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    # 2. Deteksi wajah & kumpulkan crop\n",
    "    t0 = time.perf_counter()\n",
    "    all_faces = []\n",
    "    meta = []\n",
    "    for idx, (img_path, img_bgr, img_rgb) in enumerate(zip(img_paths, imgs_bgr, imgs_rgb)):\n",
    "        detections = detect_faces(\n",
    "            img_path=img_bgr,\n",
    "            threshold=threshold,\n",
    "            model=detector,\n",
    "        )\n",
    "\n",
    "        # Pastikan format list of dict\n",
    "        if isinstance(detections, dict) and all(isinstance(v, dict) for v in detections.values()):\n",
    "            detections = list(detections.values())\n",
    "\n",
    "        for det in detections:\n",
    "            coords = det[\"facial_area\"]\n",
    "            if isinstance(coords, list):\n",
    "                x1, y1, x2, y2 = coords\n",
    "            else:  # dict\n",
    "                x1, y1 = coords[\"x\"], coords[\"y\"]\n",
    "                x2, y2 = x1 + coords[\"w\"], y1 + coords[\"h\"]\n",
    "\n",
    "            face_conf = det.get(\"confidence\", det.get(\"score\", 0.0))\n",
    "            crop = img_rgb[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            # Resize & normalize\n",
    "            crop_resized = preprocessing.resize_image(crop, target_size=(160, 160))\n",
    "            crop_norm = preprocessing.normalize_input(crop_resized, normalization=normalization)\n",
    "\n",
    "            if crop_norm.ndim == 4 and crop_norm.shape[0] == 1:\n",
    "                crop_norm = crop_norm[0]\n",
    "\n",
    "            all_faces.append(crop_norm)\n",
    "            meta.append({\n",
    "                \"filename\": os.path.basename(img_path),\n",
    "                \"facial_area\": {\n",
    "                    \"x\": int(x1), \"y\": int(y1),\n",
    "                    \"w\": int(x2 - x1), \"h\": int(y2 - y1)\n",
    "                },\n",
    "                \"confidence\": float(face_conf)\n",
    "            })\n",
    "    print(f\"[TIMING] Face detection + preprocessing: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    if not all_faces:\n",
    "        print(\"[INFO] No faces detected.\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Convert ke batch array\n",
    "    t0 = time.perf_counter()\n",
    "    batch_faces = np.array(all_faces)\n",
    "    lenfaces = len(batch_faces)\n",
    "    print(f\"[TIMING] Convert to batch array: {(time.perf_counter() - t0):.6f} sec\")\n",
    "    print(f\"[INFO] Processing {lenfaces} faces in batch...\")\n",
    "\n",
    "    # 4. Sekali forward pass ke Facenet\n",
    "    t0 = time.perf_counter()\n",
    "    embeddings_list = []\n",
    "    for i, face in enumerate(batch_faces):\n",
    "        face_tensor = tf.convert_to_tensor(face[None, ...])\n",
    "        output_dict = trt_infer(face_tensor)\n",
    "        embedding = next(iter(output_dict.values())).numpy()\n",
    "        embeddings_list.append(embedding)\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    print(f\"[TIMING] Embedding forward pass: {(time.perf_counter() - t0):.4f} sec\")\n",
    "    \n",
    "    # 5. Kelompokkan hasil per file\n",
    "    t0 = time.perf_counter()\n",
    "    results_by_file = {}\n",
    "    for m, emb in zip(meta, embeddings):\n",
    "        m[\"embedding\"] = emb.astype(float).tolist()\n",
    "        results_by_file.setdefault(m[\"filename\"], []).append(m)\n",
    "    print(f\"[TIMING] Grouping results: {(time.perf_counter() - t0):.6f} sec\")\n",
    "\n",
    "    print(f\"[TIMING] TOTAL runtime: {(time.perf_counter() - t0_total):.4f} sec\")\n",
    "    return results_by_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486b5ace-3cfb-4b53-a193-e25d4e98ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TIMING] Load images: 18.0196 sec\n",
      "[TIMING] Face detection + preprocessing: 37.0737 sec\n",
      "[TIMING] Convert to batch array: 0.845818 sec\n",
      "[INFO] Processing 1001 faces in batch...\n",
      "[TIMING] Embedding forward pass: 3.4455 sec\n",
      "[TIMING] Grouping results: 0.120994 sec\n",
      "[TIMING] TOTAL runtime: 59.5068 sec\n",
      "‚úÖ Batch embeddings saved to embedding/embeddings_fp16.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Contoh penggunaan\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"anjay\"\n",
    "    output_json = \"embedding/embeddings_fp16.json\"\n",
    "    os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "\n",
    "    img_files = [\n",
    "        os.path.join(input_folder, f)\n",
    "        for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    results = batch_representfp16(img_files)\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Batch embeddings saved to {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a31682-9ede-4f65-a341-68f00aebec63",
   "metadata": {},
   "source": [
    "# Pake TensorRT Retina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86388529-e8b8-4d37-9dad-06784a1553b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from retinafacelib import preprocess, postprocess\n",
    "\n",
    "def detect_faces_trt(\n",
    "    img_bgr: np.ndarray,\n",
    "    threshold: float = 0.9,\n",
    "    allow_upscaling: bool = True\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Inference dengan RetinaFace TF-TRT FP16 SavedModel.\n",
    "    Return: list of dict {score, facial_area, landmarks}\n",
    "    \"\"\"\n",
    "    resp = []\n",
    "\n",
    "    # --- 1. Preprocess\n",
    "    im_tensor, im_info, im_scale = preprocess.preprocess_image(img_bgr, allow_upscaling)\n",
    "    im_tensor = tf.convert_to_tensor(im_tensor, dtype=tf.float32)\n",
    "    print(f\"Taik -> {im_tensor.shape}\")\n",
    "\n",
    "    # --- 2. Forward pass (TF-TRT)\n",
    "    net_out = infer_detector(im_tensor)\n",
    "    print(\"berhasil 1 king\")\n",
    "    net_out = [v.numpy() for v in net_out.values()]\n",
    "    print(\"berhasil 2 king\")\n",
    "    # --- 3. Postprocess (diadaptasi dari retina.py)\n",
    "    nms_threshold = 0.4\n",
    "    decay4 = 0.5\n",
    "    _feat_stride_fpn = [32, 16, 8]\n",
    "    _anchors_fpn = {\n",
    "        \"stride32\": np.array([[-248., -248., 263., 263.],\n",
    "                              [-120., -120., 135., 135.]], dtype=np.float32),\n",
    "        \"stride16\": np.array([[-56., -56., 71., 71.],\n",
    "                              [-24., -24., 39., 39.]], dtype=np.float32),\n",
    "        \"stride8\":  np.array([[-8., -8., 23., 23.],\n",
    "                              [0., 0., 15., 15.]], dtype=np.float32),\n",
    "    }\n",
    "    _num_anchors = {\"stride32\": 2, \"stride16\": 2, \"stride8\": 2}\n",
    "\n",
    "    proposals_list, scores_list, landmarks_list = [], [], []\n",
    "    sym_idx = 0\n",
    "\n",
    "    for s in _feat_stride_fpn:\n",
    "        scores = net_out[sym_idx]\n",
    "        scores = scores[:, :, :, _num_anchors[f\"stride{s}\"]:]\n",
    "        bbox_deltas = net_out[sym_idx + 1]\n",
    "        height, width = bbox_deltas.shape[1], bbox_deltas.shape[2]\n",
    "\n",
    "        A = _num_anchors[f\"stride{s}\"]\n",
    "        K = height * width\n",
    "        anchors_fpn = _anchors_fpn[f\"stride{s}\"]\n",
    "        anchors = postprocess.anchors_plane(height, width, s, anchors_fpn)\n",
    "        anchors = anchors.reshape((K * A, 4))\n",
    "        scores = scores.reshape((-1, 1))\n",
    "\n",
    "        # bbox decode\n",
    "        bbox_stds = [1., 1., 1., 1.]\n",
    "        bbox_pred_len = bbox_deltas.shape[3] // A\n",
    "        bbox_deltas = bbox_deltas.reshape((-1, bbox_pred_len))\n",
    "        for i in range(4):\n",
    "            bbox_deltas[:, i::4] *= bbox_stds[i]\n",
    "        proposals = postprocess.bbox_pred(anchors, bbox_deltas)\n",
    "        proposals = postprocess.clip_boxes(proposals, im_info[:2])\n",
    "\n",
    "        if s == 4 and decay4 < 1.0:\n",
    "            scores *= decay4\n",
    "\n",
    "        scores_ravel = scores.ravel()\n",
    "        order = np.where(scores_ravel >= threshold)[0]\n",
    "        proposals = proposals[order, :]\n",
    "        scores = scores[order]\n",
    "        proposals[:, 0:4] /= im_scale\n",
    "\n",
    "        proposals_list.append(proposals)\n",
    "        scores_list.append(scores)\n",
    "\n",
    "        # landmark\n",
    "        landmark_deltas = net_out[sym_idx + 2]\n",
    "        landmark_pred_len = landmark_deltas.shape[3] // A\n",
    "        landmark_deltas = landmark_deltas.reshape((-1, 5, landmark_pred_len // 5))\n",
    "        landmarks = postprocess.landmark_pred(anchors, landmark_deltas)\n",
    "        landmarks = landmarks[order, :]\n",
    "        landmarks[:, :, 0:2] /= im_scale\n",
    "        landmarks_list.append(landmarks)\n",
    "\n",
    "        sym_idx += 3\n",
    "\n",
    "    if not proposals_list:\n",
    "        return resp\n",
    "\n",
    "    proposals = np.vstack(proposals_list)\n",
    "    scores = np.vstack(scores_list)\n",
    "    landmarks = np.vstack(landmarks_list)\n",
    "\n",
    "    # NMS\n",
    "    scores_ravel = scores.ravel()\n",
    "    order = scores_ravel.argsort()[::-1]\n",
    "    proposals = proposals[order, :]\n",
    "    scores = scores[order]\n",
    "    landmarks = landmarks[order].astype(np.float32, copy=False)\n",
    "\n",
    "    pre_det = np.hstack((proposals[:, 0:4], scores)).astype(np.float32, copy=False)\n",
    "    keep = postprocess.cpu_nms(pre_det, nms_threshold)\n",
    "\n",
    "    det = np.hstack((pre_det, proposals[:, 4:]))\n",
    "    det = det[keep, :]\n",
    "    landmarks = landmarks[keep]\n",
    "\n",
    "    for idx, face in enumerate(det):\n",
    "        resp.append({\n",
    "            \"score\": float(face[4]),\n",
    "            \"facial_area\": [int(face[0]), int(face[1]), int(face[2]), int(face[3])],\n",
    "            \"landmarks\": {\n",
    "                \"right_eye\": landmarks[idx][0].tolist(),\n",
    "                \"left_eye\":  landmarks[idx][1].tolist(),\n",
    "                \"nose\":      landmarks[idx][2].tolist(),\n",
    "                \"mouth_right\": landmarks[idx][3].tolist(),\n",
    "                \"mouth_left\":  landmarks[idx][4].tolist(),\n",
    "            }\n",
    "        })\n",
    "    return resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092508d0-eb4f-4ac4-b9ea-f9d55961dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_represent(\n",
    "    img_paths: List[str],\n",
    "    threshold: float = 0.9,\n",
    "    normalization: str = \"base\"\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "\n",
    "    t0_total = time.perf_counter()\n",
    "\n",
    "    # 1. Load semua gambar\n",
    "    t0 = time.perf_counter()\n",
    "    imgs_rgb = []\n",
    "    imgs_bgr = []\n",
    "    for p in img_paths:\n",
    "        img_rgb, _ = image_utils.load_image(p)\n",
    "        imgs_rgb.append(img_rgb)\n",
    "        imgs_bgr.append(img_rgb[:, :, ::-1])\n",
    "    print(f\"[TIMING] Load images: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    # 2. Deteksi wajah & crop\n",
    "    t0 = time.perf_counter()\n",
    "    all_faces = []\n",
    "    meta = []\n",
    "    for img_path, img_bgr, img_rgb in zip(img_paths, imgs_bgr, imgs_rgb):\n",
    "        print(f\"{img_path} -> {img_bgr.shape}\")\n",
    "        detections = detect_faces_trt(img_bgr, threshold=threshold)\n",
    "        # detections = detect_faces(img_path=img_bgr, threshold=threshold, model=detector)\n",
    "\n",
    "        if isinstance(detections, dict) and all(isinstance(v, dict) for v in detections.values()):\n",
    "            detections = list(detections.values())\n",
    "\n",
    "        for det in detections:\n",
    "            coords = det[\"facial_area\"]\n",
    "            if isinstance(coords, list):\n",
    "                x1, y1, x2, y2 = coords\n",
    "            else:\n",
    "                x1, y1 = coords[\"x\"], coords[\"y\"]\n",
    "                x2, y2 = x1 + coords[\"w\"], y1 + coords[\"h\"]\n",
    "\n",
    "            crop = img_rgb[int(y1):int(y2), int(x1):int(x2)]\n",
    "            crop_resized = preprocessing.resize_image(crop, target_size=(160, 160))\n",
    "            crop_norm = preprocessing.normalize_input(crop_resized, normalization=normalization)\n",
    "\n",
    "            if crop_norm.ndim == 4 and crop_norm.shape[0] == 1:\n",
    "                crop_norm = crop_norm[0]\n",
    "\n",
    "            all_faces.append(crop_norm)\n",
    "            meta.append({\n",
    "                \"filename\": os.path.basename(img_path),\n",
    "                \"facial_area\": {\"x\": int(x1), \"y\": int(y1), \"w\": int(x2 - x1), \"h\": int(y2 - y1)},\n",
    "                \"confidence\": float(det.get(\"confidence\", det.get(\"score\", 0.0)))\n",
    "            })\n",
    "    print(f\"[TIMING] Face detection + preprocessing: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    if not all_faces:\n",
    "        print(\"[INFO] No faces detected.\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Convert ke batch array\n",
    "    t0 = time.perf_counter()\n",
    "    batch_faces = np.array(all_faces, dtype=np.float32)\n",
    "    print(f\"[TIMING] Convert to batch array: {(time.perf_counter() - t0):.6f} sec\")\n",
    "    print(f\"[INFO] Processing {len(batch_faces)} faces in batch...\")\n",
    "\n",
    "    # 4. Forward pass pakai TF-TRT FP16\n",
    "    t0 = time.perf_counter()\n",
    "    embeddings_list = []\n",
    "    for i, face in enumerate(batch_faces):\n",
    "        face_tensor = tf.convert_to_tensor(face[None, ...])\n",
    "        output_dict = trt_infer(face_tensor)\n",
    "        embedding = next(iter(output_dict.values())).numpy()\n",
    "        embeddings_list.append(embedding)\n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    print(f\"[TIMING] Embedding forward pass: {(time.perf_counter() - t0):.4f} sec\")\n",
    "\n",
    "    # 5. Kelompokkan hasil\n",
    "    t0 = time.perf_counter()\n",
    "    results_by_file = {}\n",
    "    for m, emb in zip(meta, embeddings):\n",
    "        m[\"embedding\"] = emb.astype(float).tolist()\n",
    "        results_by_file.setdefault(m[\"filename\"], []).append(m)\n",
    "    print(f\"[TIMING] Grouping results: {(time.perf_counter() - t0):.6f} sec\")\n",
    "\n",
    "    print(f\"[TIMING] TOTAL runtime: {(time.perf_counter() - t0_total):.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018860a1-2fa1-49b7-bcb0-e82fd4b91074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Contoh penggunaan\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = \"anjay\"\n",
    "    output_json = \"embedding/embeddings_fp16_all.json\"\n",
    "    os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "\n",
    "    img_files = [\n",
    "        os.path.join(input_folder, f)\n",
    "        for f in os.listdir(input_folder)\n",
    "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    results = batch_represent(img_files)\n",
    "\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Batch embeddings saved to {output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25a3e6-1196-409d-99c0-f40f8b8f5685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
